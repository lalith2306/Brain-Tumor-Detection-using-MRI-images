# -*- coding: utf-8 -*-
"""Brain Tumor detection using mri images.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YJysbYKxShTNmnG5Da_Pvma4LIN-ja-0
"""

wasfrom google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d sartajbhuvaji/brain-tumor-classification-mri

from zipfile import ZipFile
file_name = "/content/brain-tumor-classification-mri.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')

!pip install --upgrade tensorflow

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalMaxPooling2D,Flatten, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
import os
import numpy as np

import os
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Assuming you have defined 'image_size' and 'batch_size'
image_size = 224
batch_size = 192

# Define the directory for your training dataset
train_dir = "/content/Training"

# Create an ImageDataGenerator for the training dataset with data augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # 20% of the data will be used for validation
)

# Create a training data generator
train_data = train_datagen.flow_from_directory(
    batch_size=batch_size,
    directory=train_dir,
    shuffle=True,
    target_size=(image_size, image_size),
    class_mode="categorical",
    subset="training"  # Use this subset for training
)

# Create a validation data generator
validation_data = train_datagen.flow_from_directory(
    batch_size=batch_size,
    directory=train_dir,
    shuffle=False,
    target_size=(image_size, image_size),
    class_mode="categorical",
    subset="validation"
)


# Check the number of samples in training and validation sets
print("Number of training samples:", len(train_data))
print("Number of validation samples:", len(validation_data))

image_size = 224
batch_size = 128
epochs = 5

"""### CNN"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator



# Initialize a MirroredStrategy.
strategy = tf.distribute.MirroredStrategy()

# Open a strategy scope.
with strategy.scope():
    # Build the model
    model = Sequential()

    # Convolutional layers
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size, image_size, 3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Flatten the feature maps
    model.add(Flatten())

    # Fully connected layers
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))  # Optional dropout for regularization
    model.add(Dense(128, activation='relu'))

    # Output layer
    model.add(Dense(len(train_data.class_indices), activation='softmax'))

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Display the model summary
    model.summary()

    # Define callbacks (optional)
    checkpoint = ModelCheckpoint("best_model.h5", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Train the model
history = model.fit(
    train_data,
    epochs=epochs,
    validation_data=validation_data,
    callbacks=checkpoint
)
model.save('best_model.keras')

"""### RESNET-50"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, LeakyReLU

# Load the pre-trained ResNet50 model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)

# Freeze the layers of the pre-trained ResNet50 model
for layer in base_model.layers:
    layer.trainable = False

# Build your custom model on top of the pre-trained ResNet50
model_resnet = Sequential()
model_resnet.add(base_model)
model_resnet.add(GlobalAveragePooling2D())
model_resnet.add(Dense(512))
model_resnet.add(LeakyReLU(alpha=0.01))  # LeakyReLU activation
model_resnet.add(BatchNormalization())
model_resnet.add(Dropout(0.5))

# Additional Dense layers with LeakyReLU
model_resnet.add(Dense(256))
model_resnet.add(LeakyReLU(alpha=0.01))
model_resnet.add(BatchNormalization())
model_resnet.add(Dropout(0.5))

model_resnet.add(Dense(128))
model_resnet.add(LeakyReLU(alpha=0.01))
model_resnet.add(BatchNormalization())
model_resnet.add(Dropout(0.5))

model_resnet.add(Dense(4, activation='softmax'))

# Compile the model
optimizer = Adam(learning_rate=0.0001)
model_resnet.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model_resnet.summary()

model_checkpoint_filpath3 = "ResNet50_model_checkpoint.keras"
callbacks_checkpoints3 = ModelCheckpoint(
    filepath = model_checkpoint_filpath3,
    save_weights_only = True,
    monitor = "val_accuracy",
    mode = "max",
    save_best_only = True
)

historyResNet = model_resnet.fit(train_data,
                    steps_per_epoch = len(train_data),
                    epochs = epochs,
                    validation_data = validation_data,
                    validation_steps = len(validation_data),
                    verbose=1,
                    callbacks = [callbacks_checkpoints3]
                    )

model_resnet.save('ResNet50_model.keras')

"""### MOBILE NET"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, LeakyReLU
from tensorflow.keras.applications import MobileNetV2

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)

for layer in base_model.layers:
    layer.trainable = False

# Build your custom model on top of the pre-trained vgg-19
modelMN = Sequential()
modelMN.add(base_model)
modelMN.add(GlobalAveragePooling2D())
modelMN.add(Dense(512))
modelMN.add(LeakyReLU(alpha=0.01))  # LeakyReLU activation
modelMN.add(BatchNormalization())
modelMN.add(Dropout(0.5))

# Additional Dense layers with LeakyReLU
modelMN.add(Dense(256))
modelMN.add(LeakyReLU(alpha=0.01))
modelMN.add(BatchNormalization())
modelMN.add(Dropout(0.5))

modelMN.add(Dense(128))
modelMN.add(LeakyReLU(alpha=0.01))
modelMN.add(BatchNormalization())
modelMN.add(Dropout(0.5))

modelMN.add(Dense(4, activation='softmax'))

# Compile the model
optimizer = Adam(learning_rate=0.0001)
modelMN.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

modelMN.summary()

model_checkpoint_filpath4 = "mobile_net_model_checkpoint.keras"
callbacks_checkpoints4 = ModelCheckpoint(

    filepath = model_checkpoint_filpath4,
    save_weights_only = True,
    monitor = "val_accuracy",
    mode = "max",
    save_best_only = True
)

history_MN = modelMN.fit(train_data,
                    steps_per_epoch = len(train_data),
                    epochs = epochs,
                    validation_data = validation_data,
                    validation_steps = len(validation_data),
                    verbose=1,
                    callbacks = [callbacks_checkpoints4]
                    )

modelMN.save('mobile_net.keras')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, LeakyReLU
from tensorflow.keras.applications import Xception


# Load pre-trained Xception model without the top (fully connected) layers
base_model = Xception(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)

for layer in base_model.layers:
    layer.trainable = False

# Build your custom model on top of the pre-trained Xception
model_xception = Sequential()
model_xception.add(base_model)
model_xception.add(GlobalAveragePooling2D())
model_xception.add(Dense(512))
model_xception.add(LeakyReLU(alpha=0.01))  # LeakyReLU activation
model_xception.add(BatchNormalization())
model_xception.add(Dropout(0.5))

# Additional Dense layers with LeakyReLU
model_xception.add(Dense(256))
model_xception.add(LeakyReLU(alpha=0.01))
model_xception.add(BatchNormalization())
model_xception.add(Dropout(0.5))

model_xception.add(Dense(128))
model_xception.add(LeakyReLU(alpha=0.01))
model_xception.add(BatchNormalization())
model_xception.add(Dropout(0.5))

model_xception.add(Dense(4, activation='softmax'))

# Compile the model
optimizer = Adam(learning_rate=0.0001)
model_xception.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model_xception.summary()

# Assuming 'model_xception' is your Xception model
model_checkpoint_filepath_xception = "Xception_model_checkpoint.keras"
callbacks_checkpoints_xception = ModelCheckpoint(
    filepath=model_checkpoint_filepath_xception,
    save_weights_only=True,
    monitor="val_accuracy",
    mode="max",
    save_best_only=True
)

history_xception = model_xception.fit(
    train_data,
    steps_per_epoch=len(train_data),
    epochs=epochs,
    validation_data=validation_data,
    validation_steps=len(validation_data),
    verbose=1,
    callbacks=[callbacks_checkpoints_xception]
)

model_xception.save('xception_model.keras')

# Assuming you have defined 'image_size' and the paths to your testing dataset
test_dir = "/content/Testing"

# Create an ImageDataGenerator for the testing dataset
test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)

# Flow from directory for the testing dataset
test_data = test_datagen.flow_from_directory(
    batch_size=batch_size,  # Use the same batch size as during training
    directory=test_dir,
    shuffle=False,  # Set to False to keep the order of predictions
    target_size=(image_size, image_size),
    class_mode="categorical"
)

from tensorflow.keras.models import load_model

# Load the pre-trained models
model_cnn = load_model('/content/best_model.keras')
model_resnet = load_model('/content/ResNet50_model.keras')
model_mobilenet = load_model('/content/mobile_net.keras')
model_xception = load_model('/content/xception_model.keras')

# Assuming you have test_data, adjust as needed
# Evaluate the models on the testing dataset
evaluation_cnn = model_cnn.evaluate(test_data)
evaluation_resnet = model_resnet.evaluate(test_data)
evaluation_mobilenet = model_mobilenet.evaluate(test_data)
evaluation_xception = model_xception.evaluate(test_data)

# Print the evaluation results
print("CNN Evaluation - Loss: {:.4f}, Accuracy: {:.4f}".format(evaluation_cnn[0], evaluation_cnn[1]))
print("ResNet50 Evaluation - Loss: {:.4f}, Accuracy: {:.4f}".format(evaluation_resnet[0], evaluation_resnet[1]))
print("MobileNetV2 Evaluation - Loss: {:.4f}, Accuracy: {:.4f}".format(evaluation_mobilenet[0], evaluation_mobilenet[1]))
print("Xception Evaluation - Loss: {:.4f}, Accuracy: {:.4f}".format(evaluation_xception[0], evaluation_xception[1]))

import matplotlib.pyplot as plt

output_folder = '/content/working'
# Plot training and validation loss
plt.figure(figsize=(12, 6))

plt.subplot(1, 4, 1)
plt.plot(history.history['loss'], label='CNN Training Loss')
plt.plot(history.history['val_loss'], label='CNN Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig(os.path.join(output_folder, 'CNN_Loss.png'))

plt.subplot(1, 4, 2)
plt.plot(historyResNet.history['loss'], label='ResNet Training Loss')
plt.plot(historyResNet.history['val_loss'], label='ResNet Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig(os.path.join(output_folder, 'ResNet_Loss.png'))


plt.subplot(1, 4, 3)
plt.plot(history_MN.history['loss'], label='MobileNet Training Loss')
plt.plot(history_MN.history['val_loss'], label='MobileNet Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig(os.path.join(output_folder, 'MobileNet_Loss.png'))


plt.subplot(1, 4, 4)
plt.plot(history_xception.history['loss'], label='Xception Training Loss')
plt.plot(history_xception.history['val_loss'], label='Xception Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig(os.path.join(output_folder, 'Xception_Loss.png'))

plt.show()

# Plot training and validation accuracy
plt.figure(figsize=(12, 6))

plt.subplot(1, 4, 1)
plt.plot(history.history['accuracy'], label='CNN Training Accuracy')
plt.plot(history.history['val_accuracy'], label='CNN Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(os.path.join(output_folder, 'CNN_Accuracy.png'))


plt.subplot(1, 4, 2)
plt.plot(historyResNet.history['accuracy'], label='ResNet-50 Training Accuracy')
plt.plot(historyResNet.history['val_accuracy'], label='ResNet-50 Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(os.path.join(output_folder, 'ResNet50_Accuracy.png'))


plt.subplot(1, 4, 3)
plt.plot(history_MN.history['accuracy'], label='MobileNet Training Accuracy')
plt.plot(history_MN.history['val_accuracy'], label='MobileNet Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(os.path.join(output_folder, 'MobileNet_Accuracy.png'))


plt.subplot(1, 4, 4)
plt.plot(history_xception.history['accuracy'], label='Xception Training Accuracy')
plt.plot(history_xception.history['val_accuracy'], label='Xception Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(os.path.join(output_folder, 'Xception_Accuracy.png'))


plt.show()

import matplotlib.pyplot as plt
import os

output_folder = '/content/working'

# Create a figure for loss
plt.figure(figsize=(12, 6))

# Subplot for loss
plt.subplot(1, 2, 1)

# Plot loss curves for each model
plt.plot(history.history['loss'], label='CNN Training Loss', linestyle='-', marker='o')
plt.plot(historyResNet.history['loss'], label='ResNet Training Loss', linestyle='-', marker='o')
plt.plot(history_MN.history['loss'], label='MobileNet Training Loss', linestyle='-', marker='o')
plt.plot(history_xception.history['loss'], label='Xception Training Loss', linestyle='-', marker='o')

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Comparison')
plt.legend()
plt.grid(True)

# Subplot for accuracy
plt.subplot(1, 2, 2)

# Plot accuracy curves for each model
plt.plot(history.history['accuracy'], label='CNN Training Accuracy', linestyle='-', marker='o')
plt.plot(historyResNet.history['accuracy'], label='ResNet Training Accuracy', linestyle='-', marker='o')
plt.plot(history_MN.history['accuracy'], label='MobileNet Training Accuracy', linestyle='-', marker='o')
plt.plot(history_xception.history['accuracy'], label='Xception Training Accuracy', linestyle='-', marker='o')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Comparison')
plt.legend()
plt.grid(True)

# Save the figure
plt.tight_layout()
plt.savefig(os.path.join(output_folder, 'Training_Comparison.png'))
plt.show()

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import numpy as np



# Function to get predictions and true labels
def get_predictions(model, data_generator):
    predictions = model.predict(data_generator)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = data_generator.classes
    class_labels = list(data_generator.class_indices.keys())
    return predicted_labels, true_labels, class_labels

# Get predictions and true labels for each model
predictions_cnn, true_labels_cnn, class_labels = get_predictions(model, validation_data)
predictions_resnet, true_labels_resnet, _ = get_predictions(model_resnet, validation_data)
predictions_mobilenet, true_labels_mobilenet, _ = get_predictions(model_mobilenet, validation_data)
predictions_xception, true_labels_xception, _ = get_predictions(model_xception, validation_data)

# Generate confusion matrices
conf_matrix_cnn = confusion_matrix(true_labels_cnn, predictions_cnn)
conf_matrix_resnet = confusion_matrix(true_labels_resnet, predictions_resnet)
conf_matrix_mobilenet = confusion_matrix(true_labels_mobilenet, predictions_mobilenet)
conf_matrix_xception = confusion_matrix(true_labels_xception, predictions_xception)

# Plot confusion matrices
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# CNN
sns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_title('CNN Confusion Matrix')
axes[0, 0].set_xticklabels(class_labels, rotation=45)
axes[0, 0].set_yticklabels(class_labels, rotation=0)

# ResNet
sns.heatmap(conf_matrix_resnet, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
axes[0, 1].set_title('ResNet Confusion Matrix')
axes[0, 1].set_xticklabels(class_labels, rotation=45)
axes[0, 1].set_yticklabels(class_labels, rotation=0)

# MobileNet
sns.heatmap(conf_matrix_mobilenet, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
axes[1, 0].set_title('MobileNet Confusion Matrix')
axes[1, 0].set_xticklabels(class_labels, rotation=45)
axes[1, 0].set_yticklabels(class_labels, rotation=0)

# Xception
sns.heatmap(conf_matrix_xception, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])
axes[1, 1].set_title('Xception Confusion Matrix')
axes[1, 1].set_xticklabels(class_labels, rotation=45)
axes[1, 1].set_yticklabels(class_labels, rotation=0)

# Show plots
plt.tight_layout()
plt.show()

# Generate classification reports
print("CNN Classification Report:")
print(classification_report(true_labels_cnn, predictions_cnn, target_names=class_labels))

print("ResNet Classification Report:")
print(classification_report(true_labels_resnet, predictions_resnet, target_names=class_labels))

print("MobileNet Classification Report:")
print(classification_report(true_labels_mobilenet, predictions_mobilenet, target_names=class_labels))

print("Xception Classification Report:")
print(classification_report(true_labels_xception, predictions_xception, target_names=class_labels))

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import numpy as np

# Function to get predictions and true labels
def get_predictions(model, data_generator):
    predictions = model.predict(data_generator)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = data_generator.classes
    class_labels = list(data_generator.class_indices.keys())
    return predicted_labels, true_labels, class_labels

# Get predictions and true labels for each model on the testing dataset
predictions_cnn, true_labels_cnn, _ = get_predictions(model, test_data)
predictions_resnet, true_labels_resnet, _ = get_predictions(model_resnet, test_data)
predictions_mobilenet, true_labels_mobilenet, _ = get_predictions(model_mobilenet, test_data)
predictions_xception, true_labels_xception, _ = get_predictions(model_xception, test_data)

# Generate confusion matrices
conf_matrix_cnn = confusion_matrix(true_labels_cnn, predictions_cnn)
conf_matrix_resnet = confusion_matrix(true_labels_resnet, predictions_resnet)
conf_matrix_mobilenet = confusion_matrix(true_labels_mobilenet, predictions_mobilenet)
conf_matrix_xception = confusion_matrix(true_labels_xception, predictions_xception)

# Plot confusion matrices
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# CNN
sns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_title('CNN Confusion Matrix')
axes[0, 0].set_xticklabels(class_labels, rotation=45)
axes[0, 0].set_yticklabels(class_labels, rotation=0)

# ResNet
sns.heatmap(conf_matrix_resnet, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
axes[0, 1].set_title('ResNet Confusion Matrix')
axes[0, 1].set_xticklabels(class_labels, rotation=45)
axes[0, 1].set_yticklabels(class_labels, rotation=0)

# MobileNet
sns.heatmap(conf_matrix_mobilenet, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
axes[1, 0].set_title('MobileNet Confusion Matrix')
axes[1, 0].set_xticklabels(class_labels, rotation=45)
axes[1, 0].set_yticklabels(class_labels, rotation=0)

# Xception
sns.heatmap(conf_matrix_xception, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])
axes[1, 1].set_title('Xception Confusion Matrix')
axes[1, 1].set_xticklabels(class_labels, rotation=45)
axes[1, 1].set_yticklabels(class_labels, rotation=0)

# Show plots
plt.tight_layout()
plt.show()

# Generate classification reports
print("CNN Classification Report:")
print(classification_report(true_labels_cnn, predictions_cnn, target_names=class_labels))

print("ResNet Classification Report:")
print(classification_report(true_labels_resnet, predictions_resnet, target_names=class_labels))

print("MobileNet Classification Report:")
print(classification_report(true_labels_mobilenet, predictions_mobilenet, target_names=class_labels))

print("Xception Classification Report:")
print(classification_report(true_labels_xception, predictions_xception, target_names=class_labels))

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import numpy as np
import os

# Function to get predictions and true labels
def get_predictions(model, data_generator):
    predictions = model.predict(data_generator)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = data_generator.classes
    class_labels = list(data_generator.class_indices.keys())
    return predicted_labels, true_labels, class_labels

# Get predictions and true labels for each model on the testing dataset
predictions_cnn, true_labels_cnn, _ = get_predictions(model, test_data)
predictions_resnet, true_labels_resnet, _ = get_predictions(model_resnet, test_data)
predictions_mobilenet, true_labels_mobilenet, _ = get_predictions(model_mobilenet, test_data)
predictions_xception, true_labels_xception, _ = get_predictions(model_xception, test_data)

# Generate confusion matrices
conf_matrix_cnn = confusion_matrix(true_labels_cnn, predictions_cnn)
conf_matrix_resnet = confusion_matrix(true_labels_resnet, predictions_resnet)
conf_matrix_mobilenet = confusion_matrix(true_labels_mobilenet, predictions_mobilenet)
conf_matrix_xception = confusion_matrix(true_labels_xception, predictions_xception)

# Plot confusion matrices
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# CNN
sns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
axes[0, 0].set_title('CNN Confusion Matrix')
axes[0, 0].set_xticklabels(class_labels, rotation=45)
axes[0, 0].set_yticklabels(class_labels, rotation=0)

# ResNet
sns.heatmap(conf_matrix_resnet, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])
axes[0, 1].set_title('ResNet Confusion Matrix')
axes[0, 1].set_xticklabels(class_labels, rotation=45)
axes[0, 1].set_yticklabels(class_labels, rotation=0)

# MobileNet
sns.heatmap(conf_matrix_mobilenet, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
axes[1, 0].set_title('MobileNet Confusion Matrix')
axes[1, 0].set_xticklabels(class_labels, rotation=45)
axes[1, 0].set_yticklabels(class_labels, rotation=0)

# Xception
sns.heatmap(conf_matrix_xception, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])
axes[1, 1].set_title('Xception Confusion Matrix')
axes[1, 1].set_xticklabels(class_labels, rotation=45)
axes[1, 1].set_yticklabels(class_labels, rotation=0)

# Save confusion matrices as images
output_folder = '/content/working'
plt.tight_layout()
plt.savefig(os.path.join(output_folder, 'confusion_matrices.png'))

# Save classification reports to text files
report_cnn = classification_report(true_labels_cnn, predictions_cnn, target_names=class_labels)
with open(os.path.join(output_folder, 'classification_report_cnn.txt'), 'w') as file:
    file.write(report_cnn)

report_resnet = classification_report(true_labels_resnet, predictions_resnet, target_names=class_labels)
with open(os.path.join(output_folder, 'classification_report_resnet.txt'), 'w') as file:
    file.write(report_resnet)

report_mobilenet = classification_report(true_labels_mobilenet, predictions_mobilenet, target_names=class_labels)
with open(os.path.join(output_folder, 'classification_report_mobilenet.txt'), 'w') as file:
    file.write(report_mobilenet)

report_xception = classification_report(true_labels_xception, predictions_xception, target_names=class_labels)
with open(os.path.join(output_folder, 'classification_report_xception.txt'), 'w') as file:
    file.write(report_xception)

plt.show()

import random
from PIL import Image
from keras.preprocessing.image import load_img

def plot_and_export_random_images(model, data_generator, num_images=6, output_folder='/content/working'):
    # Get random test images and labels
    random_indices = random.sample(range(len(data_generator.filenames)), num_images)
    random_images = [data_generator.filepaths[i] for i in random_indices]
    random_labels = [data_generator.classes[i] for i in random_indices]

    # Reverse class indices for mapping indices to class names
    class_indices_reverse = {v: k for k, v in data_generator.class_indices.items()}

    # Get predictions for random images
    random_data = test_datagen.flow_from_directory(
        batch_size=num_images,
        directory=test_dir,
        shuffle=False,
        target_size=(image_size, image_size),
        class_mode="categorical"
    )
    predictions = model.predict(random_data)
    predicted_labels = np.argmax(predictions, axis=1)

    # Plot and export images
    plt.figure(figsize=(15, 8))
    for i in range(num_images):
        img_path = random_images[i]
        true_label = class_indices_reverse[random_labels[i]]  # Reverse mapping here
        predicted_label = class_indices_reverse[predicted_labels[i]]

        # Load and plot the image
        img = load_img('/content/Test.jpg', target_size=(image_size, image_size))
        plt.subplot(2, 3, i + 1)
        plt.imshow(img)
        plt.title(f'True: {true_label}\nPredicted: {predicted_label}')
        plt.axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(output_folder, 'random_test_images_predictions.png'))
    plt.show()

plot_and_export_random_images(model, test_data)
plot_and_export_random_images(model_resnet, test_data)
plot_and_export_random_images(model_mobilenet, test_data)
plot_and_export_random_images(model_xception, test_data)

"""### SVM"""

from sklearn.svm import SVC
# Train the SVM model
svm_model = SVC(kernel='linear', C=1.0, random_state=42)
svm_model.fit(data_generator, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(data_generator)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix_svm = confusion_matrix(y_test, y_pred)

# Print the results
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_svm,display_labels=np.unique(y_test))

disp.plot()

"""### RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

# Tune hyperparameters and use Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=2, min_samples_leaf=1, random_state=42)
rf_model.fit(data_generator, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(data_generator)

# Evaluate the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)

# Print the results for Random Forest
print("Random Forest Accuracy:", accuracy_rf)
print("Random Forest Confusion Matrix:")
print(conf_matrix_rf)
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_rf,display_labels=np.unique(y_test))

disp.plot()

"""## GRADIENT BOOSTING"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# Train the Gradient Boosting model
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(data_generator, y_train)

# Make predictions on the test set
y_pred_gb = gb_model.predict(data_generator)

# Evaluate the Gradient Boosting model
accuracy_gb = accuracy_score(y_test, y_pred_gb)
conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)
print("Gradient Boosting Accuracy:", accuracy_gb)
print("Gradient Boosting Confusion Matrix:")
print(conf_matrix_gb)
print("Gradient Boosting Classification Report:")
print(classification_report(y_test, y_pred_gb))
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_gb,display_labels=np.unique(y_test))
disp.plot()